{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15d6829",
   "metadata": {},
   "source": [
    "# Sentence Convergence w/ NLP\n",
    "\n",
    "cosine similarity might be able to work best as the output metric to tell what idea is converging/diverging. Because it tracks the similarities between points of comparison.\n",
    "\n",
    "But then checking the length of transcript for a particular person. Do we break it up by sentence. \n",
    "- What is considered that \"smallest unit?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a710e",
   "metadata": {},
   "source": [
    "# Setting up the files for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575b089",
   "metadata": {},
   "source": [
    "## Idea 1 - Clean and reproducible - placebo\n",
    "\n",
    "This just cleans up the transcript so it is cleanly viewed. Cleans up text format. If it is empty transcript, removes row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c073c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 255 utterances for session 2021_05_21_ABI_S15_ABI\n",
      "        global_session                speaker   timestamp  global_timestamp_sec                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     transcript\n",
      "2021_05_21_ABI_S15_ABI             Brad Smith 00:00-00:37                 615.0                                                                                                                       Well, um, yeah, let's maybe start some introductions. We're beginning the obviously by the third breakout session and all of this stuff beginning to know each other pretty well. So, uh, I'll just read them out here. Well, let me I've already introduced. Oh man, I'm a chemistry based person and optical imaging is primarily what I do. So the lack of deep tissue imaging is the problem I'm trying to avoid or trying to get around. Uh, but let's just see what other modalities that we've got there. So, um, Anarude, maybe start with you because you're you're not eating.\n",
      "2021_05_21_ABI_S15_ABI             Brad Smith 00:00-00:07                 615.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         which I will do, but whoever's birthday is closest to this day, you're in trouble, so it is an honor system I suppose.\n",
      "2021_05_21_ABI_S15_ABI          Carolyn Bayer 00:00-00:52                 615.0 could be used, anything that's absorbing light or another form of energy could in theory be used to generate an acoustic signal. Um, and so the the problem we'll run into though with deep tissue imaging is sensitivity. Um, so if you had a genetic reporter, you're not likely generating sufficient concentrations at depth that you would have the sensitivity necessary to detect it. Um, you know, we think a lot though about redesigning our systems for improved signal to noise, you know, improved light delivery, you know, and all of those things can work in your favor when you're trying to really push the limits on sensitivity, but we obviously have some big goals and big challenges.\n",
      "2021_05_21_ABI_S15_ABI             Sixian You 00:00-00:23                 615.0                                                                                                                                                                                                                                                                                                                                                              sound the unscattering property, right? So if we know the signal is there, can we use ultrasound to confine the signal there, to modulate the signal, and then to get the signal back. So I think that could be very interesting and I need to learn more about Barbara's research on electrodes as wave guide. Uh, it sounds really fascinating.\n",
      "2021_05_21_ABI_S15_ABI          Carolyn Bayer 00:00-00:06                 615.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   determining how to send light in in a way that it scatters deeper in tissue.\n",
      "2021_05_21_ABI_S15_ABI             Brad Smith 00:01-00:06                 616.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Genia, you're the one that's pioneering it. You've you've\n",
      "2021_05_21_ABI_S15_ABI Yevgenia Kozorovitskiy 00:06-00:32                 621.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    I I think yes, uh, but primarily by having advantages of gentle illumination and relatively rapid access to 3D information.\n",
      "2021_05_21_ABI_S15_ABI             Brad Smith 00:07-00:34                 622.0                                                                                                                                                                                                                                                                One thing that I I've I've definitely heard some talks where people just image what they call the the early photons. So the ones that are the the least the least the least scattered. So you only collect the first 5% of the signal on the assumption that it got there first was the least scattered and so it's going to have the best resolution and you toss out 95% of the the rest of it. I don't know if that's a feasible technology.\n",
      "2021_05_21_ABI_S15_ABI             Brad Smith 00:09-00:12                 624.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Um, well,\n",
      "2021_05_21_ABI_S15_ABI          Barbara Smith 00:12-00:15                 627.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               No, nobody's going to volunteer.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your file\n",
    "df = pd.read_csv(\"/Users/maxchalekson/Projects/NICO-Research/NICO_human-gemini/Data/utterance_data_aligned_full.csv\")\n",
    "\n",
    "# --- Basic cleanup ---\n",
    "df['transcript'] = df['transcript'].astype(str).str.strip()\n",
    "df = df[df['transcript'] != \"\"]\n",
    "df = df.dropna(subset=['transcript'])\n",
    "\n",
    "# Sort chronologically\n",
    "df = df.sort_values(by=['global_session', 'global_timestamp_sec']).reset_index(drop=True)\n",
    "\n",
    "# Filter for specific session\n",
    "session_id = \"2021_05_21_ABI_S15_ABI\"\n",
    "df_session = df[df['global_session'] == session_id].reset_index(drop=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"Loaded {len(df_session)} utterances for session {session_id}\")\n",
    "\n",
    "# Show preview\n",
    "preview_cols = ['global_session', 'speaker', 'timestamp', 'global_timestamp_sec', 'transcript']\n",
    "print(df_session[preview_cols].head(10).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ad1f0",
   "metadata": {},
   "source": [
    "## Idea 2 - Sentence Level Unit-ideas\n",
    "\n",
    "(the main way how the excel file will be processed.)\n",
    "\n",
    "Deletes the backchannel utterances (e.g. \"okay\", \"yes\", \"right\", \"sounds good\")\n",
    "\n",
    "Splits up transcript by sentence by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d8d0a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-level units saved -> sentences_2021_05_21_ABI_S15.csv  (497 rows)\n",
      "global_ts_clean min/max: 615.000 / 1267.127  (~ minutes 10 → 21)\n",
      "clips included (unique): [15.0] ...\n",
      "      speaker  global_ts_clean   dialogue_act                                                                                                                    sentence\n",
      "   Brad Smith            615.0 Proposal/Offer                                                                       Well, um, yeah, let's maybe start some introductions.\n",
      "   Brad Smith            615.0  Inform/Report We're beginning the obviously by the third breakout session and all of this stuff beginning to know each other pretty well.\n",
      "   Brad Smith            615.0  Inform/Report                                                                                       So, uh, I'll just read them out here.\n",
      "   Brad Smith            615.0  Inform/Report                                                                                       Well, let me I've already introduced.\n",
      "   Brad Smith            615.0  Inform/Report                                            Oh man, I'm a chemistry based person and optical imaging is primarily what I do.\n",
      "   Brad Smith            615.0  Inform/Report                              So the lack of deep tissue imaging is the problem I'm trying to avoid or trying to get around.\n",
      "   Brad Smith            615.0 Proposal/Offer                                                          Uh, but let's just see what other modalities that we've got there.\n",
      "   Brad Smith            615.0  Inform/Report                                                     So, um, Anarude, maybe start with you because you're you're not eating.\n",
      "Aniruddha Ray            653.0  Inform/Report                                                                                                         Uh, hello everyone.\n",
      "Aniruddha Ray            653.0  Inform/Report                                                                           So I've actually spoken to everyone here already.\n",
      "   Brad Smith            661.0  Inform/Report                                   Maybe focus on your modality because I think when you get that's where we're going to be.\n",
      "   Brad Smith            661.0  Inform/Report Either your problem that you're trying to solve in deep tissue or the modality that you think you can bring to the problem.\n"
     ]
    }
   ],
   "source": [
    "# === End-to-end (stitched): Clean -> Global time across clips -> Sentence units (+ dialogue acts) ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "csv_path   = \"/Users/maxchalekson/Projects/NICO-Research/NICO_human-gemini/Data/utterance_data_aligned_full.csv\"\n",
    "session_key_prefix = r\"^2021_05_21_ABI_S15\"   # regex to match all clips for this session\n",
    "out_name  = \"sentences_2021_05_21_ABI_S15.csv\"\n",
    "\n",
    "# -------------------- LOAD + BASIC CLEAN --------------------\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"transcript\"] = df[\"transcript\"].astype(str).str.strip()\n",
    "df = df[df[\"transcript\"] != \"\"].dropna(subset=[\"transcript\"])\n",
    "\n",
    "if \"global_session\" not in df.columns:\n",
    "    raise ValueError(\"Expected column 'global_session' not found.\")\n",
    "\n",
    "# Keep ALL clips that belong to this session (prefix match)\n",
    "mask_session = df[\"global_session\"].astype(str).str.match(session_key_prefix, na=False)\n",
    "df_session = df[mask_session].copy()\n",
    "if df_session.empty:\n",
    "    raise ValueError(\"No rows found for the selected session prefix. Check 'global_session' values.\")\n",
    "\n",
    "# Make numeric\n",
    "for c in [\"start_sec\",\"end_sec\",\"clip_offset_sec\",\"global_timestamp_sec\",\"clip_number\"]:\n",
    "    if c in df_session.columns:\n",
    "        df_session[c] = pd.to_numeric(df_session[c], errors=\"coerce\")\n",
    "\n",
    "# -------------------- BUILD STITCHED GLOBAL TIMESTAMP --------------------\n",
    "def build_global_ts_clean_stitched(df_sess: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Prefer true stitched time = clip_offset_sec + start_sec.\n",
    "    If clip_offset_sec is missing/NaN, synthesize offsets by ordering clip_number\n",
    "    and accumulating each clip's max(end_sec).\n",
    "    Enforce strictly non-decreasing timeline with tiny nudges.\n",
    "    \"\"\"\n",
    "    idx = df_sess.index\n",
    "    start = df_sess.get(\"start_sec\")\n",
    "    offset = df_sess.get(\"clip_offset_sec\")\n",
    "\n",
    "    # If clip_offset_sec is mostly missing, synthesize from clip_number blocks\n",
    "    if (offset is None) or (offset.isna().mean() > 0.5):\n",
    "        if \"clip_number\" not in df_sess.columns:\n",
    "            # fallback to best-effort: use global_timestamp_sec if present\n",
    "            base = pd.to_numeric(df_sess.get(\"global_timestamp_sec\"), errors=\"coerce\")\n",
    "            ts = base.fillna(start).fillna(0.0)\n",
    "        else:\n",
    "            # order clips and accumulate durations\n",
    "            tmp = df_sess[[\"clip_number\",\"start_sec\",\"end_sec\"]].copy()\n",
    "            # robust per-clip duration (>= 0)\n",
    "            clip_stats = tmp.groupby(\"clip_number\").agg(\n",
    "                clip_min=(\"start_sec\",\"min\"),\n",
    "                clip_max=(\"end_sec\",\"max\"),\n",
    "            )\n",
    "            clip_stats[\"clip_dur\"] = (clip_stats[\"clip_max\"] - clip_stats[\"clip_min\"]).clip(lower=0).fillna(0)\n",
    "            # sort clips as they appear\n",
    "            ordered_clips = sorted(clip_stats.index.dropna())\n",
    "            cum = 0.0\n",
    "            offsets = {}\n",
    "            for cn in ordered_clips:\n",
    "                offsets[cn] = cum\n",
    "                cum += float(clip_stats.loc[cn, \"clip_dur\"])\n",
    "            # stitched time = synthesized_offset + (start - clip_min) within that clip\n",
    "            within = df_sess[\"start_sec\"] - df_sess.groupby(\"clip_number\")[\"start_sec\"].transform(\"min\")\n",
    "            ts = within.fillna(0.0) + df_sess[\"clip_number\"].map(offsets).fillna(0.0)\n",
    "    else:\n",
    "        # We have usable offsets: stitched time = offset + start\n",
    "        ts = offset.fillna(0.0) + start.fillna(0.0)\n",
    "\n",
    "    # Enforce strictly non-decreasing\n",
    "    eps = 1e-3\n",
    "    ts = pd.to_numeric(ts, errors=\"coerce\").fillna(0.0).astype(float)\n",
    "    prev = -np.inf\n",
    "    fixed = []\n",
    "    for v in ts:\n",
    "        if v <= prev:\n",
    "            v = prev + eps\n",
    "        fixed.append(v)\n",
    "        prev = v\n",
    "\n",
    "    return pd.Series(fixed, index=df_sess.index, dtype=\"float64\")\n",
    "\n",
    "df_session[\"global_ts_clean\"] = build_global_ts_clean_stitched(df_session)\n",
    "\n",
    "# -------------------- SENTENCE SPLIT + DIALOGUE ACTS --------------------\n",
    "BACKCHANNELS = {\n",
    "    \"ok\",\"okay\",\"okay.\",\"ok.\",\"yes\",\"yeah\",\"yep\",\"right\",\"mm-hmm\",\"mhm\",\"uh-huh\",\n",
    "    \"sounds good\",\"sounds good.\",\"sure\",\"great\",\"thanks\",\"thank you\",\"cool\",\"nice\",\n",
    "    \"works for me\",\"fine by me\",\"let's do it\",\"lets do it\",\"i'm in\",\"im in\"\n",
    "}\n",
    "PROPOSAL_PATTERNS = [\n",
    "    r\"\\blet'?s\\b\",\n",
    "    r\"\\b(shall we|should we|could we|can we)\\b\",\n",
    "    r\"\\b(do you want to|dya want to|wanna)\\b\",\n",
    "    r\"\\b(how about|what if|why don'?t we)\\b\",\n",
    "]\n",
    "ACCEPTANCE_PATTERNS = [\n",
    "    r\"\\b(sounds good|sounds great|works for me|fine by me|that works|all good)\\b\",\n",
    "    r\"^(ok|okay|yes|yeah|yep|sure|alright)\\b\",\n",
    "    r\"\\b(let'?s do it|count me in|i'?m in|im in|sgtm)\\b\",\n",
    "]\n",
    "REJECTION_PATTERNS = [r\"\\b(no|not now|maybe later|can'?t|cannot|won'?t|don'?t think so)\\b\"]\n",
    "QUESTION_MARKERS  = [r\"\\?$\", r\"^(who|what|when|where|why|how)\\b\"]\n",
    "ABBR_TERMINALS    = {\"mr.\",\"mrs.\",\"ms.\",\"dr.\",\"prof.\",\"sr.\",\"jr.\",\"vs.\",\"etc.\",\"e.g.\",\"i.e.\"}\n",
    "\n",
    "def naive_split(text: str):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text: return []\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    parts = [p for p in parts if p]\n",
    "    merged = [parts[0]] if parts else []\n",
    "    for p in parts[1:]:\n",
    "        prev = merged[-1]\n",
    "        prev_tail = prev.strip().lower().split()[-1] if prev.strip() else \"\"\n",
    "        if prev_tail in ABBR_TERMINALS:\n",
    "            merged[-1] = prev + \" \" + p\n",
    "        else:\n",
    "            merged.append(p)\n",
    "    return [m.strip() for m in merged if m.strip()]\n",
    "\n",
    "def split_into_sentences(text: str):\n",
    "    return naive_split(text)\n",
    "\n",
    "def is_backchannel(s: str) -> bool:\n",
    "    t = re.sub(r\"[^\\w\\s?.!-]\", \"\", (s or \"\").lower()).strip()\n",
    "    return (t in BACKCHANNELS) or (len(t) <= 12 and t in {\"ok\",\"okay\",\"yes\",\"yeah\",\"yep\",\"right\",\"cool\",\"nice\"})\n",
    "\n",
    "def attach_intra_row_backchannels(sent_list):\n",
    "    if not sent_list: return []\n",
    "    cleaned, bc_buf = [], []\n",
    "    for s in sent_list:\n",
    "        if is_backchannel(s):\n",
    "            if cleaned: cleaned[-1] = cleaned[-1] + \" \" + s\n",
    "            else: bc_buf.append(s)\n",
    "        else:\n",
    "            if bc_buf:\n",
    "                s = \" \".join(bc_buf) + \" \" + s\n",
    "                bc_buf = []\n",
    "            cleaned.append(s)\n",
    "    if bc_buf and cleaned: cleaned[-1] = cleaned[-1] + \" \" + \" \".join(bc_buf)\n",
    "    elif bc_buf and not cleaned: cleaned = [\" \".join(bc_buf)]\n",
    "    return cleaned\n",
    "\n",
    "def any_match(patterns, s):\n",
    "    s_l = (s or \"\").lower()\n",
    "    return any(re.search(p, s_l) for p in patterns)\n",
    "\n",
    "def classify_dialogue_act(sentence: str) -> str:\n",
    "    s = (sentence or \"\").strip()\n",
    "    s_l = s.lower()\n",
    "    if any_match(ACCEPTANCE_PATTERNS, s_l): return \"Acceptance\"\n",
    "    if any_match(REJECTION_PATTERNS, s_l):  return \"Rejection/Deferral\"\n",
    "    if any_match(PROPOSAL_PATTERNS, s_l) and not re.search(r\"\\blet me\\b\", s_l): return \"Proposal/Offer\"\n",
    "    if any_match(QUESTION_MARKERS, s_l):    return \"Question\"\n",
    "    return \"Inform/Report\"\n",
    "\n",
    "STOP = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"so\",\"to\",\"of\",\"in\",\"on\",\"for\",\"with\",\"at\",\"by\",\n",
    "    \"is\",\"am\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"it\",\"this\",\"that\",\"these\",\"those\",\n",
    "    \"i\",\"you\",\"he\",\"she\",\"we\",\"they\",\"me\",\"him\",\"her\",\"us\",\"them\",\"my\",\"your\",\"his\",\"her\",\"our\",\"their\"\n",
    "}\n",
    "def content_signature(text, k=3):\n",
    "    toks = re.findall(r\"[A-Za-z0-9']+\", (text or \"\").lower())\n",
    "    toks = [t for t in toks if t not in STOP]\n",
    "    seen, sig = set(), []\n",
    "    for t in toks:\n",
    "        if t not in seen:\n",
    "            sig.append(t); seen.add(t)\n",
    "        if len(sig) >= k: break\n",
    "    return \" \".join(sig)\n",
    "\n",
    "# -------------------- BUILD SENTENCE-LEVEL UNITS --------------------\n",
    "df_session = df_session.sort_values([\"clip_number\",\"start_sec\",\"end_sec\"], na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "rows = []\n",
    "for idx, r in df_session.iterrows():\n",
    "    sents = attach_intra_row_backchannels(split_into_sentences(r[\"transcript\"]))\n",
    "    for j, s in enumerate(sents, start=1):\n",
    "        rows.append({\n",
    "            \"global_session\": r.get(\"global_session\"),\n",
    "            \"session\": r.get(\"session\"),\n",
    "            \"clip_number\": r.get(\"clip_number\"),\n",
    "            \"speaker\": r.get(\"speaker\"),\n",
    "            \"timestamp\": r.get(\"timestamp\"),\n",
    "            \"global_ts_clean\": r.get(\"global_ts_clean\"),\n",
    "            \"start_sec\": r.get(\"start_sec\"),\n",
    "            \"end_sec\": r.get(\"end_sec\"),\n",
    "            \"source_row_index\": idx,\n",
    "            \"sent_index_in_row\": j,\n",
    "            \"sentence\": s,\n",
    "            \"dialogue_act\": classify_dialogue_act(s),\n",
    "            \"content_signature\": content_signature(s, k=3)\n",
    "        })\n",
    "\n",
    "df_sentences = pd.DataFrame(rows).sort_values(\n",
    "    [\"global_ts_clean\",\"source_row_index\",\"sent_index_in_row\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# -------------------- SAVE + AUDIT --------------------\n",
    "df_sentences.to_csv(out_name, index=False)\n",
    "mmin = float(df_sentences[\"global_ts_clean\"].min())\n",
    "mmax = float(df_sentences[\"global_ts_clean\"].max())\n",
    "print(f\"Sentence-level units saved -> {out_name}  ({len(df_sentences)} rows)\")\n",
    "print(f\"global_ts_clean min/max: {mmin:.3f} / {mmax:.3f}  (~ minutes {int(mmin//60)} → {int(mmax//60)})\")\n",
    "print(f\"clips included (unique): {sorted(pd.unique(df_session['clip_number'].dropna()))[:10]} ...\")\n",
    "print(df_sentences[[\"speaker\",\"global_ts_clean\",\"dialogue_act\",\"sentence\"]].head(12).to_string(index=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "275ad341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_ts_clean range: 615.000 → 1267.127 sec  (~10.9 min)\n",
      "non‑monotonic steps: 0   |   gaps >5min: 0\n",
      "\n",
      "Per‑clip coverage on stitched timeline:\n",
      "               n  ts_min    ts_max\n",
      "clip_number                       \n",
      "15.0         497   615.0  1267.127\n",
      "\n",
      "Distinct global_session values (first 20):\n",
      "global_session\n",
      "2021_05_21_ABI_S15_ABI    497\n",
      "\n",
      "EARLY:\n",
      " global_ts_clean  clip_number    speaker   dialogue_act                                                                         sentence\n",
      "           615.0         15.0 Brad Smith Proposal/Offer                            Well, um, yeah, let's maybe start some introductions.\n",
      "           615.0         15.0 Brad Smith  Inform/Report We're beginning the obviously by the third breakout session and all of this s...\n",
      "           615.0         15.0 Brad Smith  Inform/Report                                            So, uh, I'll just read them out here.\n",
      "\n",
      "MIDDLE:\n",
      " global_ts_clean  clip_number                speaker  dialogue_act                                                                         sentence\n",
      "        1267.007         15.0          Carolyn Bayer Inform/Report                                                                     Well absorb.\n",
      "        1267.008         15.0 Yevgenia Kozorovitskiy Inform/Report You're just the same argument made for pH sensing uh probes last uh breakout ...\n",
      "        1267.009         15.0    Alexandra Dickinson    Acceptance                                              Yeah, that's that's where we're at.\n",
      "\n",
      "LATE:\n",
      " global_ts_clean  clip_number      speaker  dialogue_act                                                                         sentence\n",
      "        1267.125         15.0   Brad Smith Inform/Report It's uh these sessions are always a little draining when everyone's staring a...\n",
      "        1267.126         15.0 Silvia Ronco Inform/Report                                                                                .\n",
      "        1267.127         15.0 Silvia Ronco    Acceptance                                                       Okay, are we all here now?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = \"/Users/maxchalekson/Projects/NICO-Research/NICO_human-gemini/Data/sentences_2021_05_21_ABI_S15.csv\"\n",
    "s = pd.read_csv(path)\n",
    "\n",
    "# ---- Basic coverage ----\n",
    "tmin, tmax = float(s[\"global_ts_clean\"].min()), float(s[\"global_ts_clean\"].max())\n",
    "dur_min = (tmax - tmin) / 60.0\n",
    "print(f\"global_ts_clean range: {tmin:.3f} → {tmax:.3f} sec  (~{dur_min:.1f} min)\")\n",
    "\n",
    "# Expect ~0 → ~3997 sec (≈ 66.6 min). If you see ~615→1267 (~10.9 min), you only captured one clip.\n",
    "\n",
    "# ---- Monotonicity & big gaps (clip seams) ----\n",
    "s = s.sort_values([\"global_ts_clean\",\"source_row_index\",\"sent_index_in_row\"]).reset_index(drop=True)\n",
    "diffs = s[\"global_ts_clean\"].diff()\n",
    "non_mono = int((diffs < 0).sum())\n",
    "big_gaps = s.loc[diffs > 300, \"global_ts_clean\"]  # >5 min jumps\n",
    "print(f\"non‑monotonic steps: {non_mono}   |   gaps >5min: {len(big_gaps)}\")\n",
    "\n",
    "# ---- Clip coverage & order sanity ----\n",
    "# (These columns came from the source rows, so they’ll be NaN if your source lacked them.)\n",
    "for c in [\"clip_number\",\"start_sec\",\"end_sec\"]:\n",
    "    if c not in s.columns:\n",
    "        s[c] = np.nan\n",
    "\n",
    "clip_span = (s.groupby(\"clip_number\")\n",
    "               .agg(n=(\"sentence\",\"count\"),\n",
    "                    ts_min=(\"global_ts_clean\",\"min\"),\n",
    "                    ts_max=(\"global_ts_clean\",\"max\"))\n",
    "               .sort_values(\"ts_min\"))\n",
    "print(\"\\nPer‑clip coverage on stitched timeline:\")\n",
    "print(clip_span.to_string())\n",
    "\n",
    "# ---- Are we really getting *all* the clips for the session prefix? ----\n",
    "# If ts range is too short, check how many distinct global_session IDs survived:\n",
    "print(\"\\nDistinct global_session values (first 20):\")\n",
    "print(s[\"global_session\"].dropna().astype(str).value_counts().head(20).to_string())\n",
    "\n",
    "# ---- Spot check: early, middle, late rows ----\n",
    "def show(rows):\n",
    "    cols = [\"global_ts_clean\",\"clip_number\",\"speaker\",\"dialogue_act\",\"sentence\"]\n",
    "    print(s.loc[rows, cols].to_string(index=False, max_colwidth=80))\n",
    "\n",
    "n = len(s)\n",
    "print(\"\\nEARLY:\")\n",
    "show(range(3))\n",
    "print(\"\\nMIDDLE:\")\n",
    "show(range(max(0,n//2-2), min(n, n//2+1)))\n",
    "print(\"\\nLATE:\")\n",
    "show(range(max(0,n-3), n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41c47c",
   "metadata": {},
   "source": [
    "Couple of things done here:\n",
    "\n",
    "1. Breaking the excel files' row into sentences\n",
    "\n",
    "In the raw file, one \"row\" might be a short filler (\"Yeah.\") or a long paragraph with multiple ideas. That makes it hard to comapre ideas fairly. So we split each row into sentences. Now every chunk is about the same size, which makes our simililarity scores more meaningful.\n",
    "\n",
    "2. Keeping the speaker\n",
    "\n",
    "We always keep track of who said it. This way we can tell if an idea is picked up by someone-else (cross-speaker) or if the same person is just adding more details (same-speaker)\n",
    "\n",
    "3. Tagging the type of sentence\n",
    "\n",
    "We give each sentence a quick label like `Proposal/Offer`, `Acceptance`, `Question`, or `Inform/Report`. This is important because not every sentence is an \"idea\". For example, \"Sounds good.\" is an acceptane, not a new idea. Tagging them stops these from messing up our idea similarity results, while still letting us track decisions.\n",
    "\n",
    "4. Making the timeline cleans\n",
    "\n",
    "Sometimes timestamps in the fiel go backwards or have ties. We fix this by nudging times forward by 0.001 seconds if needed. This tiny change doesn't affect the analysis, but it keeps our data in the right order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d077c38",
   "metadata": {},
   "source": [
    "### implementing co-similarity (SBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687625ce",
   "metadata": {},
   "source": [
    "all of this still tests the idea if Gemini is even capable of defining a convergence to solution, even if NLP topics are used within the transcript, based on all this code run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "629fdb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable units for cosine: 393 (of 497)\n",
      "Candidate pairs after constraints: 23056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/conv-clean/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence candidates (>= 0.68): 21 | decision-like B: 5\n",
      "Divergence candidates (<= 0.35): 222\n",
      "Saved:\n",
      " - convergence_pairs_w120_k3_t0.68_labeled.csv\n",
      " - divergence_pairs_w120_k3_t0.35.csv\n",
      " - temporal_uptake_series_w120_k3_t0.68.csv\n",
      " - temporal_uptake_series_annotated_w120_k3_t0.68.csv\n",
      " - convergence_chains_w120_k3_t0.68_labeled.csv\n",
      "\n",
      "=== Top convergence (labeled; highest cosine) (n=15) ===\n",
      "[0.775] Δt=49s | Yevgenia Kozorovitskiy → Brad Smith | Acts: Inform/Report→Question | decision_B=True\n",
      "  A: That kind of moves us a bit away from uh considering deep tissue applications specifically.\n",
      "  B: I mean is that is that a technology that would work at a deeper tissue as well?\n",
      "\n",
      "[0.758] Δt=49s | Carolyn Bayer → Aniruddha Ray | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: So I'm spatial resolution is more in the 100 to 200 micron.\n",
      "  B: Um, or or even a few micron resolution.\n",
      "\n",
      "[0.757] Δt=67s | Brad Smith → Aniruddha Ray | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: So the lack of deep tissue imaging is the problem I'm trying to avoid or trying to get around.\n",
      "  B: And that's what is my interest in deep tissue imaging.\n",
      "\n",
      "[0.754] Δt=48s | Carolyn Bayer → Aniruddha Ray | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: alternatively, I mean just to describe what how we get around the limitation in photoacoustic imaging is that\n",
      "  B: So in addition to what Barbara said, actually if you try to do photoacoustic imaging on tissues or on animals, you'll see that there is a lot of photoacoustic signal from every layer.\n",
      "\n",
      "[0.750] Δt=49s | Carolyn Bayer → Brad Smith | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: So we we so there's there's essentially two different strategies for photoacoustic imaging.\n",
      "  B: Maybe a second bullet point is uh photoacoustic imaging saves the day.\n",
      "\n",
      "[0.746] Δt=96s | Steve Jett → Carolyn Bayer | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: So for me deep tissue imaging means I'm looking at a piece of tissue excised from somewhere deep in an organism.\n",
      "  B: I was kind of my question as I was listening is that um, are we only thinking about deep tissue imaging in the context of optics and it seems like it's a very, you know, optics microscopy heavy group, but um, obviously thinking about our colleagues that work in other imaging modalities like MRI.\n",
      "\n",
      "[0.737] Δt=48s | Carolyn Bayer → Aniruddha Ray | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: So we we so there's there's essentially two different strategies for photoacoustic imaging.\n",
      "  B: So in addition to what Barbara said, actually if you try to do photoacoustic imaging on tissues or on animals, you'll see that there is a lot of photoacoustic signal from every layer.\n",
      "\n",
      "[0.731] Δt=48s | Carolyn Bayer → Sixian You | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: Um, you know, I do photoacoustics which is using um optics to excite to then ultrasound acoustics to receive.\n",
      "  B: Um photoacoustic is a perfect genius example of doing this.\n",
      "\n",
      "[0.729] Δt=77s | Brad Smith → Alexandra Dickinson | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: Either your problem that you're trying to solve in deep tissue or the modality that you think you can bring to the problem.\n",
      "  B: Um, but I'm always really interested in deep tissue uh problems.\n",
      "\n",
      "[0.726] Δt=111s | Sixian You → Luke Mortensen | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: The first aspect is uh, um multiphoton imaging.\n",
      "  B: Um, and I um am primarily working with uh multiphoton imaging.\n",
      "\n",
      "[0.709] Δt=49s | Carolyn Bayer → Aniruddha Ray | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: So, um, my spatial resolution is not going to be even close to what all the optical microscopists um can do.\n",
      "  B: Um, or or even a few micron resolution.\n",
      "\n",
      "[0.705] Δt=48s | Carolyn Bayer → Aniruddha Ray | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: alternatively, I mean just to describe what how we get around the limitation in photoacoustic imaging is that\n",
      "  B: But then again with photoacoustics your resolution is not as good as optical purely optical methods.\n",
      "\n",
      "[0.703] Δt=67s | Brad Smith → Aniruddha Ray | Acts: Inform/Report→Inform/Report | decision_B=True\n",
      "  A: Oh man, I'm a chemistry based person and optical imaging is primarily what I do.\n",
      "  B: So I work on nanotechnology aided optical imaging, which involves fluorescence as well as photoacoustics.\n",
      "\n",
      "[0.699] Δt=56s | Aniruddha Ray → Alexandra Dickinson | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: And that's what is my interest in deep tissue imaging.\n",
      "  B: Um, but I'm always really interested in deep tissue uh problems.\n",
      "\n",
      "[0.698] Δt=49s | Carolyn Bayer → Brad Smith | Acts: Inform/Report→Inform/Report | decision_B=True\n",
      "  A: So we we so there's there's essentially two different strategies for photoacoustic imaging.\n",
      "  B: maybe shouldn't use that phrase, but it's photoacoustic imaging has promise, I don't know, has has promise and challenges.\n",
      "\n",
      "\n",
      "=== Top divergence (lowest cosine) (n=15) ===\n",
      "[0.018] Δt=94s | Luke Mortensen → Steve Jett | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: Um, so two photon and you know, a bunch of towards three photon, you know, etc. And um, uh trying to um move as well both on the fluorescence side, um as well as in the second harmonic, third harmonic generation.\n",
      "  B: I'm I'm the fly on the wall. Okay.\n",
      "\n",
      "[0.038] Δt=94s | Luke Mortensen → Steve Jett | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: Um, and uh Athens Georgia, um biomedical engineering background.\n",
      "  B: I'm I'm the fly on the wall. Okay.\n",
      "\n",
      "[0.059] Δt=60s | Luke Mortensen → Steve Jett | Acts: Question→Inform/Report | decision_B=False\n",
      "  A: You guys can hear me?\n",
      "  B: So for me deep tissue imaging means I'm looking at a piece of tissue excised from somewhere deep in an organism.\n",
      "\n",
      "[0.060] Δt=60s | Luke Mortensen → Steve Jett | Acts: Question→Inform/Report | decision_B=False\n",
      "  A: You guys can hear me?\n",
      "  B: Uh my background is in electron and atomic force microscopy.\n",
      "\n",
      "[0.083] Δt=94s | Luke Mortensen → Steve Jett | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: Um, and I um am primarily working with uh multiphoton imaging.\n",
      "  B: I'm I'm the fly on the wall. Okay.\n",
      "\n",
      "[0.121] Δt=61s | Sixian You → Luke Mortensen | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: Uh, that's one aspect.\n",
      "  B: Um, and I um am primarily working with uh multiphoton imaging.\n",
      "\n",
      "[0.121] Δt=102s | Sixian You → Steve Jett | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: And look, there's a way to cheat cheat it.\n",
      "  B: I'm I'm the fly on the wall. Okay.\n",
      "\n",
      "[0.123] Δt=111s | Sixian You → Luke Mortensen | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: Uh, so, uh, we are very much troubled by deep tissue imaging because uh, we're we're blessed with this submicro resolution but we can rarely go deeper than millimeter uh depth.\n",
      "  B: I use my computer.\n",
      "\n",
      "[0.126] Δt=54s | Barbara Smith → Yevgenia Kozorovitskiy | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: In in full person.\n",
      "  B: Um, two is you are in a light sheet type uh context then, you know, do some uh holography for shaping of beams.\n",
      "\n",
      "[0.136] Δt=52s | Sixian You → Luke Mortensen | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: And look, there's a way to cheat cheat it.\n",
      "  B: Um and um so I guess I'll continue.\n",
      "\n",
      "[0.137] Δt=8s | Sixian You → Luke Mortensen | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: And look, there's a way to cheat cheat it.\n",
      "  B: I use my computer.\n",
      "\n",
      "[0.141] Δt=48s | Barbara Smith → Brad Smith | Acts: Inform/Report→Question | decision_B=False\n",
      "  A: In in full person.\n",
      "  B: Uh yeah, Genia, you're can you talk now?\n",
      "\n",
      "[0.148] Δt=94s | Luke Mortensen → Steve Jett | Acts: Inform/Report→Inform/Report | decision_B=False\n",
      "  A: I use my computer.\n",
      "  B: I'm I'm the fly on the wall. Okay.\n",
      "\n",
      "[0.149] Δt=3s | Brad Smith → Alexandra Dickinson | Acts: Question→Inform/Report | decision_B=False\n",
      "  A: Jazz, can you what's your modality?\n",
      "  B: Like something along those lines, although I've never done it.\n",
      "\n",
      "[0.149] Δt=110s | Brad Smith → Yevgenia Kozorovitskiy | Acts: Question→Inform/Report | decision_B=False\n",
      "  A: Jazz, can you what's your modality?\n",
      "  B: Um, two is you are in a light sheet type uh context then, you know, do some uh holography for shaping of beams.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/lphw45ds14n5t74zwjdfybx40000gn/T/ipykernel_91491/3709752082.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pairs_topK[\"decision_B\"] = pairs_topK[\"sentence_B\"].apply(lambda s: bool(dec_re.search(str(s))))\n",
      "/var/folders/17/lphw45ds14n5t74zwjdfybx40000gn/T/ipykernel_91491/3709752082.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pairs_topK[\"decision_A\"] = pairs_topK[\"sentence_A\"].apply(lambda s: bool(dec_re.search(str(s))))\n"
     ]
    }
   ],
   "source": [
    "# ==== Convergence vs Divergence (refined; meta-filter, Top-K, chains, decision labels) ====\n",
    "import pandas as pd, numpy as np, re, glob, os\n",
    "\n",
    "# --- Config ---\n",
    "session_csv    = \"/Users/maxchalekson/Projects/NICO-Research/NICO_human-gemini/Data/sentences_2021_05_21_ABI_S15_ABI.csv\"\n",
    "model_name     = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Pairing policy\n",
    "window_s   = 120\n",
    "min_dt     = 2.0\n",
    "cross_only = True\n",
    "\n",
    "# Text filters\n",
    "min_tokens   = 3\n",
    "exclude_acts = {\"Acceptance\"}\n",
    "\n",
    "# Meta-utterance filter\n",
    "META_PATTERNS = [\n",
    "    r\"\\bsorry\\b\", r\"\\bchange location\\b\", r\"\\bcan you hear\\b\", r\"\\bi can hear\\b\",\n",
    "    r\"\\bare you there\\b\", r\"\\bi am here\\b\", r\"\\bmy name is\\b\", r\"\\bhello\\b\", r\"\\bhi\\b\",\n",
    "    r\"\\bmid bite\\b\", r\"\\btesting\\b\", r\"\\btest\\b\"\n",
    "]\n",
    "meta_re = re.compile(\"|\".join(META_PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "# Scoring policy\n",
    "topK_per_A = 3\n",
    "tau_conv   = 0.68\n",
    "tau_div    = 0.35\n",
    "\n",
    "N_preview  = 15\n",
    "\n",
    "# --- Load & prefilter ---\n",
    "df = pd.read_csv(session_csv)\n",
    "req = {\"global_session\",\"speaker\",\"sentence\",\"global_ts_clean\",\"dialogue_act\",\"source_row_index\",\"sent_index_in_row\"}\n",
    "missing = req - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = \"\".join(ch for ch in s if ch.isalnum() or ch.isspace())\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "df[\"sentence\"] = df[\"sentence\"].astype(str).str.strip()\n",
    "df[\"tok_len\"]  = df[\"sentence\"].str.split().apply(len)\n",
    "df[\"is_meta\"]  = df[\"sentence\"].apply(lambda s: bool(meta_re.search(s)))\n",
    "\n",
    "mask_len = df[\"tok_len\"] >= min_tokens\n",
    "mask_act = ~df[\"dialogue_act\"].isin(exclude_acts) if exclude_acts else True\n",
    "\n",
    "df_use = df[mask_len & mask_act & (~df[\"is_meta\"])].copy()\n",
    "df_use[\"norm\"] = df_use[\"sentence\"].apply(norm_text)\n",
    "\n",
    "df_use = df_use.sort_values([\"global_session\",\"global_ts_clean\",\"source_row_index\",\"sent_index_in_row\"]).reset_index(drop=True)\n",
    "df_use[\"unit_id\"] = (\n",
    "    df_use[\"global_session\"].astype(str) + \":\" +\n",
    "    df_use[\"source_row_index\"].astype(str) + \":\" +\n",
    "    df_use[\"sent_index_in_row\"].astype(str)\n",
    ")\n",
    "\n",
    "print(f\"Usable units for cosine: {len(df_use)} (of {len(df)})\")\n",
    "\n",
    "keep = [\"global_session\",\"unit_id\",\"speaker\",\"sentence\",\"norm\",\"global_ts_clean\",\"dialogue_act\",\"source_row_index\"]\n",
    "L = df_use[keep].rename(columns={\n",
    "    \"unit_id\":\"unit_A\",\"speaker\":\"speaker_A\",\"sentence\":\"sentence_A\",\"norm\":\"norm_A\",\n",
    "    \"global_ts_clean\":\"tA\",\"dialogue_act\":\"act_A\",\"source_row_index\":\"row_A\"\n",
    "})\n",
    "R = df_use[keep].rename(columns={\n",
    "    \"unit_id\":\"unit_B\",\"speaker\":\"speaker_B\",\"sentence\":\"sentence_B\",\"norm\":\"norm_B\",\n",
    "    \"global_ts_clean\":\"tB\",\"dialogue_act\":\"act_B\",\"source_row_index\":\"row_B\"\n",
    "})\n",
    "\n",
    "pairs = L.merge(R, on=\"global_session\", how=\"inner\")\n",
    "pairs = pairs[(pairs[\"tB\"] >= pairs[\"tA\"]) & (pairs[\"unit_A\"] != pairs[\"unit_B\"])]\n",
    "pairs[\"dt\"] = (pairs[\"tB\"] - pairs[\"tA\"]).astype(float)\n",
    "pairs = pairs[(pairs[\"dt\"] <= window_s) & (pairs[\"dt\"] >= min_dt)]\n",
    "pairs[\"cross_speaker\"] = pairs[\"speaker_A\"].ne(pairs[\"speaker_B\"])\n",
    "if cross_only:\n",
    "    pairs = pairs[pairs[\"cross_speaker\"]]\n",
    "pairs = pairs[pairs[\"norm_A\"] != pairs[\"norm_B\"]]\n",
    "\n",
    "print(f\"Candidate pairs after constraints: {len(pairs)}\")\n",
    "\n",
    "# --- SBERT embeddings + cosine ---\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Install sentence-transformers: pip install -q sentence-transformers\") from e\n",
    "\n",
    "model = SentenceTransformer(model_name)\n",
    "embA = model.encode(pairs[\"sentence_A\"].tolist(), convert_to_tensor=True, normalize_embeddings=True)\n",
    "embB = model.encode(pairs[\"sentence_B\"].tolist(), convert_to_tensor=True, normalize_embeddings=True)\n",
    "pairs[\"cosine\"] = (embA * embB).sum(dim=1).detach().cpu().numpy()\n",
    "\n",
    "# --- Top-K per A ---\n",
    "pairs = pairs.sort_values([\"unit_A\",\"cosine\"], ascending=[True, False])\n",
    "pairs_topK = pairs.groupby(\"unit_A\", as_index=False).head(topK_per_A)\n",
    "\n",
    "# --- Decision-language labels (toward solution) ---\n",
    "DECISION_PATTERNS = [\n",
    "    r\"\\b(let'?s|we should|we ought|we can|we could)\\b\",\n",
    "    r\"\\b(decide(?:d)?|agreed|agreement|consensus)\\b\",\n",
    "    r\"\\b(plan|next step|action item|assign|deadline)\\b\",\n",
    "    r\"\\b(so the solution|the solution is|this works|that works)\\b\",\n",
    "    r\"\\b(we'?ll|we will|we are going to|i will|i can)\\b\",\n",
    "    r\"\\b(implement|try this|do this|use that)\\b\"\n",
    "]\n",
    "dec_re = re.compile(\"|\".join(DECISION_PATTERNS), flags=re.IGNORECASE)\n",
    "pairs_topK[\"decision_B\"] = pairs_topK[\"sentence_B\"].apply(lambda s: bool(dec_re.search(str(s))))\n",
    "pairs_topK[\"decision_A\"] = pairs_topK[\"sentence_A\"].apply(lambda s: bool(dec_re.search(str(s))))\n",
    "\n",
    "# --- Split once (after labeling) ---\n",
    "conv = pairs_topK[pairs_topK[\"cosine\"] >= tau_conv].copy()\n",
    "div  = pairs_topK[pairs_topK[\"cosine\"] <= tau_div].copy()\n",
    "conv[\"conv_toward_solution\"] = conv[\"decision_B\"] | (~conv[\"decision_A\"] & conv[\"decision_B\"])\n",
    "\n",
    "print(f\"Convergence candidates (>= {tau_conv}): {len(conv)} | decision-like B: {int(conv['decision_B'].sum())}\")\n",
    "print(f\"Divergence candidates (<= {tau_div}): {len(div)}\")\n",
    "\n",
    "# --- Temporal series for plotting ---\n",
    "def to_minute(x):\n",
    "    try: return int(float(x)//60)\n",
    "    except: return None\n",
    "\n",
    "conv[\"minute_A\"] = conv[\"tA\"].apply(to_minute)\n",
    "conv[\"minute_B\"] = conv[\"tB\"].apply(to_minute)\n",
    "\n",
    "series_conv = conv.groupby(\"minute_B\").size().rename(\"conv_count\").reset_index()\n",
    "series_conv_dec = conv[conv[\"decision_B\"]].groupby(\"minute_B\").size().rename(\"conv_decision_count\").reset_index()\n",
    "\n",
    "if not series_conv.empty:\n",
    "    mmin, mmax = series_conv[\"minute_B\"].min(), series_conv[\"minute_B\"].max()\n",
    "    timeline = pd.DataFrame({\"minute_B\": list(range(mmin, mmax+1))})\n",
    "    temporal = (timeline.merge(series_conv, how=\"left\", on=\"minute_B\")\n",
    "                        .merge(series_conv_dec, how=\"left\", on=\"minute_B\")\n",
    "                        .fillna(0))\n",
    "    temporal[\"conv_rate\"] = temporal[\"conv_count\"] / max(temporal[\"conv_count\"].sum(), 1)\n",
    "else:\n",
    "    temporal = pd.DataFrame(columns=[\"minute_B\",\"conv_count\",\"conv_decision_count\",\"conv_rate\"])\n",
    "\n",
    "# --- Optional: attach human minute-notes (summary_..._minXX.txt) ---\n",
    "def read_summary_snippets(pattern=\"summary_*.txt\"):\n",
    "    rows = []\n",
    "    for p in glob.glob(pattern):\n",
    "        m = re.search(r\"_min(\\d+)\\.txt$\", os.path.basename(p))\n",
    "        if not m: continue\n",
    "        minute = int(m.group(1))\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as fh: txt = fh.read().strip()\n",
    "        except Exception: txt = \"\"\n",
    "        rows.append({\"minute_B\": minute, \"summary_note\": txt, \"summary_file\": os.path.basename(p)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "summ_df = read_summary_snippets(\"summary_*.txt\")\n",
    "if not summ_df.empty:\n",
    "    temporal_annot = temporal.merge(summ_df, how=\"left\", on=\"minute_B\")\n",
    "else:\n",
    "    temporal_annot = temporal.copy()\n",
    "\n",
    "# --- Save outputs ---\n",
    "cols = [\n",
    "    \"global_session\",\n",
    "    \"unit_A\",\"speaker_A\",\"tA\",\"act_A\",\"sentence_A\",\n",
    "    \"unit_B\",\"speaker_B\",\"tB\",\"act_B\",\"sentence_B\",\n",
    "    \"dt\",\"cosine\",\"cross_speaker\"\n",
    "]\n",
    "conv_out = conv[cols + [\"decision_A\",\"decision_B\",\"conv_toward_solution\"]]\n",
    "conv_path = f\"convergence_pairs_w{window_s}_k{topK_per_A}_t{tau_conv}_labeled.csv\"\n",
    "div_path  = f\"divergence_pairs_w{window_s}_k{topK_per_A}_t{tau_div}.csv\"\n",
    "chain_path= f\"convergence_chains_w{window_s}_k{topK_per_A}_t{tau_conv}_labeled.csv\"\n",
    "ts_path   = f\"temporal_uptake_series_w{window_s}_k{topK_per_A}_t{tau_conv}.csv\"\n",
    "ts_ann    = f\"temporal_uptake_series_annotated_w{window_s}_k{topK_per_A}_t{tau_conv}.csv\"\n",
    "\n",
    "conv_out.to_csv(conv_path, index=False)\n",
    "div[cols].to_csv(div_path, index=False)\n",
    "temporal.to_csv(ts_path, index=False)\n",
    "temporal_annot.to_csv(ts_ann, index=False)\n",
    "\n",
    "if not conv.empty:\n",
    "    conv_chain = (conv.groupby([\"unit_A\",\"speaker_A\",\"tA\",\"act_A\",\"sentence_A\"])\n",
    "                      .agg(n_responders=(\"speaker_B\",\"nunique\"),\n",
    "                           n_links=(\"unit_B\",\"nunique\"),\n",
    "                           latest_dt=(\"dt\",\"max\"),\n",
    "                           max_cosine=(\"cosine\",\"max\"),\n",
    "                           any_decision_B=(\"decision_B\",\"any\"))\n",
    "                      .reset_index()\n",
    "                      .sort_values([\"n_responders\",\"n_links\",\"max_cosine\"], ascending=[False,False,False]))\n",
    "    conv_chain.to_csv(chain_path, index=False)\n",
    "\n",
    "print(f\"Saved:\\n - {conv_path}\\n - {div_path}\\n - {ts_path}\\n - {ts_ann}\" + (f\"\\n - {chain_path}\" if not conv.empty else \"\"))\n",
    "\n",
    "# --- Preview AFTER labeling ---\n",
    "def preview(dfp, title, n=N_preview, reverse=False):\n",
    "    if dfp.empty:\n",
    "        print(f\"\\n=== {title}: (none) ===\"); return\n",
    "    dfp = dfp.sort_values(\"cosine\", ascending=reverse).head(n)\n",
    "    print(f\"\\n=== {title} (n={len(dfp)}) ===\")\n",
    "    for _, r in dfp.iterrows():\n",
    "        print(f\"[{r['cosine']:.3f}] Δt={int(r['dt'])}s | {r['speaker_A']} → {r['speaker_B']} | Acts: {r['act_A']}→{r['act_B']} | decision_B={r.get('decision_B', False)}\")\n",
    "        print(f\"  A: {r['sentence_A']}\")\n",
    "        print(f\"  B: {r['sentence_B']}\\n\")\n",
    "\n",
    "preview(conv, \"Top convergence (labeled; highest cosine)\", n=N_preview, reverse=False)\n",
    "preview(div,  \"Top divergence (lowest cosine)\",           n=N_preview, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7723acb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_ts_clean min/max: 615.0 1267.126999999997\n",
      "minutes span: 10 → 21\n",
      "\n",
      "Rows per clip_number (min/max ts):\n",
      "            start_sec              end_sec              global_ts_clean  \\\n",
      "                  min    max count     min    max count             min   \n",
      "clip_number                                                               \n",
      "15.0              0.0  652.0   497     6.0  675.0   497           615.0   \n",
      "\n",
      "                             \n",
      "                  max count  \n",
      "clip_number                  \n",
      "15.0         1267.127   497  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "sentences_path = \"/Users/maxchalekson/Projects/NICO-Research/NICO_human-gemini/Data/sentences_2021_05_21_ABI_S15_ABI.csv\"\n",
    "s = pd.read_csv(sentences_path)\n",
    "\n",
    "print(\"global_ts_clean min/max:\", float(s[\"global_ts_clean\"].min()), float(s[\"global_ts_clean\"].max()))\n",
    "print(\"minutes span:\", int(s[\"global_ts_clean\"].min()//60), \"→\", int(s[\"global_ts_clean\"].max()//60))\n",
    "\n",
    "print(\"\\nRows per clip_number (min/max ts):\")\n",
    "cols = [c for c in [\"clip_number\",\"start_sec\",\"end_sec\",\"global_ts_clean\"] if c in s.columns]\n",
    "print(s[cols].groupby(\"clip_number\").agg([\"min\",\"max\",\"count\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b88193",
   "metadata": {},
   "source": [
    "## Idea 3 - merge small speaker runs - unecessary?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
